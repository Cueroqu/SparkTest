<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>


<configuration>

    <!-- Hive Configuration can either be stored in this file or in the hadoop configuration files  -->
    <!-- that are implied by Hadoop setup variables.                                                -->
    <!-- Aside from Hadoop setup variables - this file is provided as a convenience so that Hive    -->
    <!-- users do not have to edit hadoop configuration files (that may be managed as a centralized -->
    <!-- resource).                                                                                 -->

    <!-- Start Of Hive Configuration -->
    <property>
        <name>hive.metastore.client.socket.timeout</name>
        <value>600</value>
        <final>true</final>
    </property>

    <property>
        <name>hive.mapjoin.check.memory.rows</name>
        <value>10</value>
    </property>

    <property>
        <name>hive.mapjoin.followby.map.aggr.hash.percentmemory</name>
        <value>0.1</value>
    </property>

    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://warehousestore</value>
    </property>

    <property>
        <name>hive.server2.thrift.max.worker.threads</name>
        <value>25</value>
    </property>

    <property>
        <name>mapred.reduce.tasks</name>
        <value>-1</value>
        <description>The default number of reduce tasks per job.  Typically set
            to a prime close to the number of available hosts.  Ignored when
            mapred.job.tracker is "local". Hadoop set this to 1 by default, whereas hive uses -1 as its default value.
            By setting this property to -1, Hive will automatically figure out what should be the number of reducers.
        </description>
    </property>

    <property>
        <name>hive.exec.reducers.bytes.per.reducer</name>
        <value>500000000</value>
        <description>size per reducer.The default is 1G, i.e if the input size is 10G, it will use 10 reducers.</description>
    </property>

    <property>
        <name>hive.exec.reducers.max</name>
        <value>60</value>
        <description>max number of reducers will be used. If the one
            specified in the configuration parameter mapred.reduce.tasks is
            negative, hive will use this one as the max number of reducers when
            automatically determine number of reducers.</description>
    </property>

    <property>
        <name>hive.exec.scratchdir</name>
        <value>/hive/tmp/hive-${user.name}</value>
        <description>Scratch space for Hive jobs</description>
    </property>

    <property>
        <name>hive.start.cleanup.scratchdir</name>
        <value>false</value>
        <description>To cleanup the hive scratchdir while starting the hive server</description>
    </property>

    <property>
        <name>hive.test.mode</name>
        <value>false</value>
        <description>whether hive is running in test mode. If yes, it turns on sampling and prefixes the output tablename</description>
    </property>

    <property>
        <name>hive.test.mode.prefix</name>
        <value>test_</value>
        <description>if hive is running in test mode, prefixes the output table by this string</description>
    </property>

    <!-- If the input table is not bucketed, the denominator of the tablesample is determinied by the parameter below   -->
    <!-- For example, the following query:                                                                              -->
    <!--   INSERT OVERWRITE TABLE dest                                                                                  -->
    <!--   SELECT col1 from src                                                                                         -->
    <!-- would be converted to                                                                                          -->
    <!--   INSERT OVERWRITE TABLE test_dest                                                                             -->
    <!--   SELECT col1 from src TABLESAMPLE (BUCKET 1 out of 32 on rand(1))                                             -->
    <property>
        <name>hive.test.mode.samplefreq</name>
        <value>32</value>
        <description>if hive is running in test mode and table is not bucketed, sampling frequency</description>
    </property>

    <property>
        <name>hive.test.mode.nosamplelist</name>
        <value></value>
        <description>if hive is running in test mode, dont sample the above comma seperated list of tables</description>
    </property>

    <property>
        <name>hive.metastore.uris</name>
        <value>thrift://metrics-hive-services-lb.prod.hulu.com:9083</value>
    </property>

    <property>
        <name>datanucleus.validateTables</name>
        <value>false</value>
        <description>validates existing schema against code. turn this on if you want to verify existing schema </description>
    </property>

    <property>
        <name>datanucleus.validateColumns</name>
        <value>false</value>
        <description>validates existing schema against code. turn this on if you want to verify existing schema </description>
    </property>

    <property>
        <name>datanucleus.validateConstraints</name>
        <value>false</value>
        <description>validates existing schema against code. turn this on if you want to verify existing schema </description>
    </property>

    <property>
        <name>datanucleus.storeManagerType</name>
        <value>rdbms</value>
        <description>metadata store type</description>
    </property>

    <property>
        <name>datanucleus.autoCreateSchema</name>
        <value>true</value>
        <description>creates necessary schema on a startup if one doesn't exist. set this to false, after creating it once</description>
    </property>

    <property>
        <name>datanucleus.autoStartMechanismMode</name>
        <value>checked</value>
        <description>throw exception if metadata tables are incorrect</description>
    </property>

    <property>
        <name>datancucleus.transactionIsolation</name>
        <value>read-committed</value>
        <description></description>
    </property>

    <property>
        <name>datanuclues.cache.level2</name>
        <value>true</value>
        <description>use a level 2 cache. turn this off if metadata is changed independently of hive metastore server</description>
    </property>

    <property>
        <name>datanuclues.cache.level2.type</name>
        <value>SOFT</value>
        <description>SOFT=soft reference based cache, WEAK=weak reference based cache.</description>
    </property>

    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/hive/warehouse</value>
        <description>location of default database for the warehouse</description>
    </property>

    <property>
        <name>hive.metastore.connect.retries</name>
        <value>5</value>
        <description>Number of retries while opening a connection to metastore</description>
    </property>

    <property>
        <name>hive.metastore.rawstore.impl</name>
        <value>org.apache.hadoop.hive.metastore.ObjectStore</value>
        <description>Name of the class that implements org.apache.hadoop.hive.metastore.rawstore interface. This class is used to store and retrieval of raw metadata objects such as table, database</description>
    </property>

    <property>
        <name>hive.default.fileformat</name>
        <value>TextFile</value>
        <description>Default file format for CREATE TABLE statement. Options are TextFile and SequenceFile. Users can explicitly say CREATE TABLE ... STORED AS &lt;TEXTFILE|SEQUENCEFILE&gt; to override</description>
    </property>

    <property>
        <name>hive.fileformat.check</name>
        <value>false</value>
        <description>Whether to check file format or not when loading data files</description>
    </property>

    <property>
        <name>hive.map.aggr</name>
        <value>true</value>
        <description>Whether to use map-side aggregation in Hive Group By queries</description>
    </property>

    <property>
        <name>hive.groupby.skewindata</name>
        <value>false</value>
        <description>Whether there is skew in data to optimize group by queries</description>
    </property>

    <property>
        <name>hive.groupby.mapaggr.checkinterval</name>
        <value>100000</value>
        <description>Number of rows after which size of the grouping keys/aggregation classes is performed</description>
    </property>

    <property>
        <name>hive.mapred.local.mem</name>
        <value>0</value>
        <description>For local mode, memory of the mappers/reducers</description>
    </property>

    <property>
        <name>hive.map.aggr.hash.percentmemory</name>
        <value>0.5</value>
        <description>Portion of total memory to be used by map-side grup aggregation hash table</description>
    </property>

    <property>
        <name>hive.map.aggr.hash.min.reduction</name>
        <value>0.5</value>
        <description>Hash aggregation will be turned off if the ratio between hash
            table size and input rows is bigger than this number. Set to 1 to make sure
            hash aggregation is never turned off.</description>
    </property>

    <property>
        <name>hive.optimize.cp</name>
        <value>true</value>
        <description>Whether to enable column pruner</description>
    </property>

    <property>
        <name>hive.optimize.ppd</name>
        <value>true</value>
        <description>Whether to enable predicate pushdown</description>
    </property>

    <property>
        <name>hive.optimize.pruner</name>
        <value>true</value>
        <description>Whether to enable the new partition pruner which depends on predicate pushdown. If this is disabled,
            the old partition pruner which is based on AST will be enabled.</description>
    </property>

    <property>
        <name>hive.optimize.groupby</name>
        <value>true</value>
        <description>Whether to enable the bucketed group by from bucketed partitions/tables.</description>
    </property>

    <property>
        <name>hive.join.emit.interval</name>
        <value>1000</value>
        <description>How many rows in the right-most join operand Hive should buffer before emitting the join result. </description>
    </property>

    <property>
        <name>hive.join.cache.size</name>
        <value>200000</value>
        <description>How many rows in the joining tables (except the streaming table) should be cached in memory. </description>
    </property>

    <property>
        <name>hive.mapjoin.bucket.cache.size</name>
        <value>100</value>
        <description>How many values in each keys in the map-joined table should be cached in memory. </description>
    </property>

    <property>
        <name>hive.mapjoin.maxsize</name>
        <value>100000</value>
        <description>Maximum # of rows of the small table that can be handled by map-side join. If the size is reached and hive.task.progress is set, a fatal error counter is set and the job will be killed.</description>
    </property>

    <property>
        <name>hive.mapjoin.cache.numrows</name>
        <value>25000</value>
        <description>How many rows should be cached by jdbm for map join. </description>
    </property>

    <property>
        <name>hive.mapred.mode</name>
        <value>nonstrict</value>
        <description>The mode in which the hive operations are being performed. In strict mode, some risky queries are not allowed to run</description>
    </property>

    <property>
        <name>hive.exec.script.maxerrsize</name>
        <value>100000</value>
        <description>Maximum number of bytes a script is allowed to emit to standard error (per map-reduce task). This prevents runaway scripts from filling logs partitions to capacity </description>
    </property>

    <property>
        <name>hive.exec.script.allow.partial.consumption</name>
        <value>false</value>
        <description> When enabled, this option allows a user script to exit successfully without consuming all the data from the standard input.
        </description>
    </property>

    <property>
        <name>hive.script.operator.id.env.var</name>
        <value>HIVE_SCRIPT_OPERATOR_ID</value>
        <description> Name of the environment variable that holds the unique script operator ID in the user's transform function (the custom mapper/reducer that the user has specified in the query)
        </description>
    </property>

    <property>
        <name>hive.exec.compress.output</name>
        <value>false</value>
        <description> This controls whether the final outputs of a query (to a local/hdfs file or a hive table) is compressed. The compression codec and other options are determined from hadoop config variables mapred.output.compress* </description>
    </property>

    <property>
        <name>hive.exec.parallel</name>
        <value>true</value>
        <description>Whether to execute jobs in parallel</description>
    </property>

    <property>
        <name>hive.hwi.war.file</name>
        <value>lib/hive-hwi-0.13.0.jar</value>
        <description>This sets the path to the HWI war file, relative to ${HIVE_HOME}. </description>
    </property>

    <property>
        <name>hive.hwi.listen.host</name>
        <value>metrics-hive.server.hulu.com</value>
        <description>This is the host address the Hive Web Interface will listen on</description>
    </property>

    <property>
        <name>hive.hwi.listen.port</name>
        <value>9999</value>
        <description>This is the port the Hive Web Interface will listen on</description>
    </property>

    <property>
        <name>hive.exec.pre.hooks</name>
        <value></value>
        <description>Pre Execute Hook for Tests</description>
    </property>

    <property>
        <name>hive.merge.mapfiles</name>
        <value>true</value>
        <description>Merge small files at the end of a map-only job</description>
    </property>

    <property>
        <name>hive.merge.mapredfiles</name>
        <value>false</value>
        <description>Merge small files at the end of any job(map only or map-reduce)</description>
    </property>

    <property>
        <name>hive.heartbeat.interval</name>
        <value>1000</value>
        <description>Send a heartbeat after this interval - used by mapjoin and filter operators</description>
    </property>

    <property>
        <name>hive.merge.size.per.task</name>
        <value>512000000</value>
        <description>Size of merged files at the end of the job</description>
    </property>

    <property>
        <name>hive.merge.smallfiles.avgsize</name>
        <value>128000000</value>
        <description>Size of merged files at the end of the job</description>
    </property>

    <property>
        <name>hive.script.auto.progress</name>
        <value>false</value>
        <description>Whether Hive Tranform/Map/Reduce Clause should automatically send progress information to TaskTracker to avoid the task getting killed because of inactivity.  Hive sends progress information when the script is outputting to stderr.  This option removes the need of periodically producing stderr messages, but users should be cautious because this may prevent infinite loops in the scripts to be killed by TaskTracker.  </description>
    </property>

    <property>
        <name>hive.script.serde</name>
        <value>org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</value>
        <description>The default serde for trasmitting input data to and reading output data from the user scripts. </description>
    </property>

    <property>
        <name>hive.script.recordreader</name>
        <value>org.apache.hadoop.hive.ql.exec.TextRecordReader</value>
        <description>The default record reader for reading data from the user scripts. </description>
    </property>

    <property>
        <name>hive.script.recordwriter</name>
        <value>org.apache.hadoop.hive.ql.exec.TextRecordWriter</value>
        <description>The default record writer for writing data to the user scripts. </description>
    </property>

    <property>
        <name>hive.input.format</name>
        <value>org.apache.hadoop.hive.ql.io.HiveInputFormat</value>
        <description>The default input format, if it is not specified, the system assigns it. It is set to HiveInputFormat for hadoop versions 17, 18 and 19, whereas it is set to CombinedHiveInputFormat for hadoop 20. The user can always overwrite it - if there is a bug in CombinedHiveInputFormat, it can always be manually set to HiveInputFormat. </description>
    </property>

    <property>
        <name>hive.udtf.auto.progress</name>
        <value>false</value>
        <description>Whether Hive should automatically send progress information to TaskTracker when using UDTF's to prevent the task getting killed because of inactivity.  Users should be cautious because this may prevent TaskTracker from killing tasks with infinte loops.  </description>
    </property>

    <property>
        <name>hive.mapred.reduce.tasks.speculative.execution</name>
        <value>false</value>
        <description>Whether speculative execution for reducers should be turned on. </description>
    </property>

    <property>
        <name>mapreduce.task.classpath.user.precedence</name>
        <value>true</value>
    </property>

    <property>
        <name>hive.exec.show.job.failure.debug.info</name>
        <value>false</value>
    </property>

    <property>
        <name>hive.exec.job.debug.capture.stacktraces</name>
        <value>false</value>
    </property>

    <property>
        <name>hive.mapjoin.smalltable.filesize</name>
        <value>987654321</value>
    </property>

    <property>
        <name>hive.auto.convert.join</name>
        <value>true</value>
    </property>

    <property>
        <name>hive.exec.compress.intermediate</name>
        <value>true</value>
        <description> This controls whether intermediate files produced by hive between multiple map-reduce jobs are compressed. The compression codec and other options are determined from hadoop config variables mapred.output.compress* </description>
    </property>

    <property>
        <name>hive.optimize.skewjoin</name>
        <value>false</value>
    </property>

    <property>
        <name>hive.skewjoin.key</name>
        <value>1000000</value>
    </property>

    <!-- End Of Hive Configuration -->

    <!-- Start Of HDFS Configurations -->
    <property>
        <name>dfs.nameservices</name>
        <value>warehousestore,beaconstore,apstore</value>
    </property>
    <property>
        <name>dfs.client.failover.proxy.provider.warehousestore</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <property>
        <name>dfs.ha.automatic-failover.enabled.warehousestore</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.ha.namenodes.warehousestore</name>
        <value>namenode144,namenode700</value>
    </property>
    <property>
        <name>dfs.client.failover.proxy.provider.beaconstore</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <property>
        <name>dfs.ha.namenodes.beaconstore</name>
        <value>namenode21,namenode475</value>
    </property>
    <property>
        <name>dfs.client.failover.proxy.provider.apstore</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <property>
        <name>dfs.ha.automatic-failover.enabled.apstore</name>
        <value>true</value>
    </property>
    <property>
        <name>ha.zookeeper.quorum</name>
        <value>elshadoop-h17.server.hulu.com:2181,elshadoop-i02.prod.hulu.com:2181,elshadoop-j01.prod.hulu.com:2181,elshadoop-m01.server.hulu.com:2181,elshadoop-n01.server.hulu.com:2181</value>
    </property>
    <property>
        <name>dfs.ha.namenodes.apstore</name>
        <value>namenode1732,namenode1782</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.warehousestore.namenode144</name>
        <value>elsharpynn001.prod.hulu.com:8020</value>
    </property>
    <property>
        <name>dfs.namenode.servicerpc-address.warehousestore.namenode144</name>
        <value>elsharpynn001.prod.hulu.com:8022</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.warehousestore.namenode144</name>
        <value>elsharpynn001.prod.hulu.com:50070</value>
    </property>
    <property>
        <name>dfs.namenode.https-address.warehousestore.namenode144</name>
        <value>elsharpynn001.prod.hulu.com:50470</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.warehousestore.namenode700</name>
        <value>elsharpynn003.server.hulu.com:8020</value>
    </property>
    <property>
        <name>dfs.namenode.servicerpc-address.warehousestore.namenode700</name>
        <value>elsharpynn003.server.hulu.com:8022</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.warehousestore.namenode700</name>
        <value>elsharpynn003.server.hulu.com:50070</value>
    </property>
    <property>
        <name>dfs.namenode.https-address.warehousestore.namenode700</name>
        <value>elsharpynn003.server.hulu.com:50470</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.beaconstore.namenode21</name>
        <value>elshadoopnn001.prod.hulu.com:8020</value>
    </property>
    <property>
        <name>dfs.namenode.servicerpc-address.beaconstore.namenode21</name>
        <value>elshadoopnn001.prod.hulu.com:8022</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.beaconstore.namenode21</name>
        <value>elshadoopnn001.prod.hulu.com:50070</value>
    </property>
    <property>
        <name>dfs.namenode.https-address.beaconstore.namenode21</name>
        <value>elshadoopnn001.prod.hulu.com:50470</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.beaconstore.namenode475</name>
        <value>elshadoopnn003.server.hulu.com:8020</value>
    </property>
    <property>
        <name>dfs.namenode.servicerpc-address.beaconstore.namenode475</name>
        <value>elshadoopnn003.server.hulu.com:8022</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.beaconstore.namenode475</name>
        <value>elshadoopnn003.server.hulu.com:50070</value>
    </property>
    <property>
        <name>dfs.namenode.https-address.beaconstore.namenode475</name>
        <value>elshadoopnn003.server.hulu.com:50470</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.apstore.namenode1732</name>
        <value>elsapnn001.server.hulu.com:8020</value>
    </property>
    <property>
        <name>dfs.namenode.servicerpc-address.apstore.namenode1732</name>
        <value>elsapnn001.server.hulu.com:8022</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.apstore.namenode1732</name>
        <value>elsapnn001.server.hulu.com:50070</value>
    </property>
    <property>
        <name>dfs.namenode.https-address.apstore.namenode1732</name>
        <value>elsapnn001.server.hulu.com:50470</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.apstore.namenode1782</name>
        <value>elsapnn002.server.hulu.com:8020</value>
    </property>
    <property>
        <name>dfs.namenode.servicerpc-address.apstore.namenode1782</name>
        <value>elsapnn002.server.hulu.com:8022</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.apstore.namenode1782</name>
        <value>elsapnn002.server.hulu.com:50070</value>
    </property>
    <property>
        <name>dfs.namenode.https-address.apstore.namenode1782</name>
        <value>elsapnn002.server.hulu.com:50470</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>dfs.blocksize</name>
        <value>134217728</value>
    </property>
    <property>
        <name>dfs.client.use.datanode.hostname</name>
        <value>false</value>
    </property>
    <property>
        <name>fs.permissions.umask-mode</name>
        <value>002</value>
    </property>
    <property>
        <name>dfs.namenode.acls.enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.client.read.shortcircuit</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.domain.socket.path</name>
        <value>/var/run/hdfs-sockets/dn</value>
    </property>
    <property>
        <name>dfs.client.read.shortcircuit.skip.checksum</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.client.domain.socket.data.traffic</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
        <value>true</value>
    </property>
    <!-- END Of HDFS Configurations -->
</configuration>
